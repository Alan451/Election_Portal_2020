{% load static %}
<!DOCTYPE html>
<html lang="en">
<head>
    <title>Voter verification</title>
    <script type="text/javascript" src="http://code.jquery.com/jquery-1.7.1.min.js"></script>
    <script type="text/javascript" src="{% static 'js/camvas.js' %}"></script>
    <script type="text/javascript" src="{% static 'js/Ipoc.js' %}"></script>
    <script type="text/javascript" src="{% static 'js/pico.js' %}"></script>
    <link href="https://unpkg.com/tailwindcss@^2/dist/tailwind.min.css" rel="stylesheet">
    <style type="text/css">  
        /* Flipping the video as it was not mirror view */  
        video {  
            -webkit-transform: scaleX(-1);  
            transform: scaleX(-1);  
            /* margin-top: 5px;   */
        }
      
        /* Flipping the canvas image as it was not mirror view */  
        #canvas {  
            filter: FlipH;  
            -ms-filter: "FlipH";  
            transform: scale(0.75,0.75);
        }  
    </style>
    <link rel="stylesheet" href="{% static 'css/verification.css' %}">
    
</head>
<body onload="geoFindMe();button_callback();">


    <div class="grid  grid-rows-6 grid-cols-10">
        <div id="video-resize" class="col-start-2 col-span-3 row-start-2 row-span-3">
            <p class="text-2xl custom-font">Please allow camera and location access on your device :)</p>
            <br>
            <!-- <canvas class="w-5/6 h-full left-30" style="border:solid 1px #ddd; background-color:white;" id="canvas"></canvas> -->
        </div>
        <div class="col-start-5 col-span-3 row-start-3 row-span-2">
            <p class="text-5xl custom-font">{{request.user.first_name}}</p>
            <p class="text-2xl custom-font">{{year}}  {{hostel}} hostel</p>
            <br>
            <p class="text-lg custom-font" id="status"></p>
            <p class="text-lg custom-font">IP address: {{request.session.ip}}</p>
            <p class="text-lg custom-font">Voter Token ID: {{voter_id}}</p>
        </div>
        <div class="col-start-8 sol-span-2 row-start-5">
            <button id="btnCapture" type="button" class="rounded-full bg-gray-50 w-16 h-16">
                <img class="w-14 h-7 ml-1" src="{% static 'icons/camera.svg' %}" alt="">
                <!-- <img class="w-14 h-7 ml-1" src="{% static 'icons/video.svg' %}" alt=""> -->
            </button>
            <!-- <button id="find-me" onclick=geoFindMe type="button" class="rounded-full bg-gray-50 w-16 h-16 ml-10">
                <img class="w-14 h-7 ml-1" src="{% static 'icons/location.svg' %}" alt="">
            </button> -->
        </div>
        <div class="col-start-8 row-start-6">
            <form action="" method="post"> 
                {% csrf_token %}
                {{ form }}
                <input type="submit" value="submit">
            </form>
        </div>
    </div>
            
    <!-- earlier code -->
    <!-- <h1>location verification</h1>-->
        
    <!-- <button class="btn btn-primary" id="btnCapture">Click</button>     -->
             
            <div class="col-lg-6">    
                <h4>    
                    Captured image from Webcam <input type="button" class="btn btn-primary" id="btnSave" name="btnSave" value="Save the canvas(image) to server"/>     
                </h4>       
               <canvas style="border:solid 1px #ddd;background-color:white;" id="canvas" width="640" height="480"></canvas>
            </div>    
    <script>
        var initialized = false;
        var faceDetected = false;
        function button_callback() {
			/*
				(0) chseck whether we're already running face detection
			*/
			if(initialized)
				return; // if yes, then do not initialize everything again
			/*
				(1) initialize the pico.js face detector
			*/
			var update_memory = pico.instantiate_detection_memory(5); // we will use the detecions of the last 5 frames
			var facefinder_classify_region = function(r, c, s, pixels, ldim) {return -1.0;};
			var cascadeurl = "{% static 'js/facefinder' %}";
			fetch(cascadeurl).then(function(response) {
				response.arrayBuffer().then(function(buffer) {
					var bytes = new Int8Array(buffer);
					facefinder_classify_region = pico.unpack_cascade(bytes);
					console.log('* facefinder loaded');
				})
			})
			/*
				(2) initialize the lploc.js library with a pupil localizer
			*/
			var do_puploc = function(r, c, s, nperturbs, pixels, nrows, ncols, ldim) {return [-1.0, -1.0];};
			var puplocurl = "{% static 'js/puploc.bin' %}"
			fetch(puplocurl).then(function(response) {
				response.arrayBuffer().then(function(buffer) {
					var bytes = new Int8Array(buffer);
					do_puploc = lploc.unpack_localizer(bytes);
					console.log('* puploc loaded');
				})
			})
			/*
				(3) get the drawing context on the canvas and define a function to transform an RGBA image to grayscale
			*/
			var ctx = document.getElementById('canvas').getContext('2d');
			function rgba_to_grayscale(rgba, nrows, ncols) {
				var gray = new Uint8Array(nrows*ncols);
				for(var r=0; r<nrows; ++r)
					for(var c=0; c<ncols; ++c)
						// gray = 0.2*red + 0.7*green + 0.1*blue
						gray[r*ncols + c] = (2*rgba[r*4*ncols+4*c+0]+7*rgba[r*4*ncols+4*c+1]+1*rgba[r*4*ncols+4*c+2])/10;
				return gray;
			}
			/*
				(4) this function is called each time a video frame becomes available
			*/
			var processfn = function(video, dt) {
                // render the video frame to the canvas element and extract RGBA pixel data
                
                var c = document.getElementById('canvas');
                ctx.drawImage(video, 0, 0);
				var rgba = ctx.getImageData(0, 0,640,480).data;
				// prepare input to `run_cascade`
				image = {
					"pixels": rgba_to_grayscale(rgba, 480, 640),
					"nrows": 480,
					"ncols": 640,
					"ldim": 640
				}
				params = {
					"shiftfactor": 0.1, // move the detection window by 10% of its size
					"minsize": 100,     // minimum size of a face
					"maxsize": 1000,    // maximum size of a face
					"scalefactor": 1.1  // for multiscale processing: resize the detection window by 10% when moving to the higher scale
				}
				// run the cascade over the frame and cluster the obtained detections
				// dets is an array that contains (r, c, s, q) quadruplets
				// (representing row, column, scale and detection score)
				dets = pico.run_cascade(image, facefinder_classify_region, params);
				dets = update_memory(dets);
				dets = pico.cluster_detections(dets, 0.2); // set IoU threshold to 0.2
				// draw detections
				for(i=0; i<dets.length; ++i)
					// check the detection score
					// if it's above the threshold, draw it
					// (the constant 50.0 is empirical: other cascades might require a different one)
					if(dets[i][3]>50.0)
					{
						var r, c, s;
						faceDetected = true;
						// ctx.beginPath();
						ctx.strokeRect(dets[i][1]-0.45*dets[i][2], dets[i][0]-0.45*dets[i][2], dets[i][2],dets[i][2]);
						ctx.lineWidth = 1;
						ctx.strokeStyle = 'orange';
						ctx.stroke();
						//
						// find the eye pupils for each detected face
						// starting regions for localization are initialized based on the face bounding box
						// (parameters are set empirically)
						// first eye
						r = dets[i][0] - 0.075*dets[i][2];
						c = dets[i][1] - 0.175*dets[i][2];
						s = 0.35*dets[i][2];
						[r, c] = do_puploc(r, c, s, 63, image)
						if(r>=0 && c>=0)
						{
							// ctx.beginPath();
							// ctx.arc(c, r, 1, 0, 2*Math.PI, false);
							// ctx.lineWidth = 3;
							// ctx.strokeStyle = 'red';
							// ctx.stroke();
						}
						// second eye
						r = dets[i][0] - 0.075*dets[i][2];
						c = dets[i][1] + 0.175*dets[i][2];
						s = 0.35*dets[i][2];
						[r, c] = do_puploc(r, c, s, 63, image)
						if(r>=0 && c>=0)
						{
							// ctx.beginPath();
							// ctx.arc(c, r, 1, 0, 2*Math.PI, false);
							// ctx.lineWidth = 3;
							// ctx.strokeStyle = 'red';
							// ctx.stroke();
						}
					}
			}
			/*
				(5) instantiate camera handling (see https://github.com/cbrandolino/camvas)
			*/
			var mycamvas = new camvas(ctx, processfn);
			/*
				(6) it seems that everything went well
			*/
			initialized = true;
		}
	</script>
   <script type="text/javascript">  
      var pic = false;
        function sleep(ms) {
            return new Promise(resolve => setTimeout(resolve, ms));
        }
      // Below code to capture image from Video tag (Webcam streaming)  
      $("#btnCapture").click(async function () {  
          console.log("clicked");
          var canvas = document.getElementById('canvas');  
          var context = canvas.getContext('2d');  
    
          // Capture the image into canvas from Webcam streaming Video element  
          context.drawImage(video, 0, 0);  
          if(pic){
            $('#video').get(0).play();
            // document.getElementById("btnCapture").innerHTML = "Click";
            
            pic=false;
          }
          else{
            faceDetected=false;
            await sleep(100);//if the last face detected is before 100 ms dont allow to click
            if (faceDetected==false){
              alert('No face in video')
            }  
            else{
                
                $('#video').get(0).pause();
                // document.getElementById("btnCapture").innerHTML = "Reclick";
                pic = true;
            }
          }
      }); 
  
      // Upload image to server - ajax call - with the help of base64 data as a parameter  
     
      $("#btnSave").click(function () {  
    
        
          // Below new canvas to generate flip/mirron image from existing canvas  
          var destinationCanvas = document.createElement("canvas");  
          var destCtx = destinationCanvas.getContext('2d');  
    
    
          destinationCanvas.height = 500;  
          destinationCanvas.width = 500;  
    
          destCtx.translate(video.videoWidth, 0);  
          destCtx.scale(-1, 1);  
          destCtx.drawImage(document.getElementById("canvas"), 0, 0);  
    
          // Get base64 data to send to server for upload  
          var imagebase64data = destinationCanvas.toDataURL("image/png");
          var allCookies = document.cookie;
          var csrf = allCookies.substring(10);
          imagebase64data = imagebase64data.replace('data:image/png;base64,', '');  
          var postdata = {
            imagebase64data: imagebase64data,
            'csrfmiddlewaretoken': csrf
          };
          $.post('/image/', postdata);
      });  
  </script>   

<!-- location verification javascript -->

<script>
    function post_req(data){
        var allCookies = document.cookie;
        var csrf = allCookies.substring(10);
        var postdata = {
            data: JSON.stringify(data),
            'csrfmiddlewaretoken': csrf
        };
        $.post('/geo/', postdata);
    }   
    function geoFindMe() {
        const status = document.querySelector('#status');
        
        function success(position) {
        const latitude  = position.coords.latitude;
        const longitude = position.coords.longitude;
        data = {
                        lat: latitude,
                        long: longitude,
                        }
        
        post_req(data);              
        status.textContent = 'GeoLocation : (lat : ' +latitude+', long : ' + longitude + ')';
        
        }

        function error() {
        status.textContent = 'Unable to retrieve your location';
            
        }

        if(!navigator.geolocation) {
        status.textContent = 'Geolocation is not supported by your browser';
        } else {
            status.textContent = 'GeoLocation : Locating ... ';
        navigator.geolocation.getCurrentPosition(success, error);
        }

    }

    document.querySelector('#find-me').addEventListener('click', geoFindMe);
    
</script>

</body>
</html>